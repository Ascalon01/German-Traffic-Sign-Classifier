{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figsize_default = plt.rcParams['figure.figsize']\n",
    "mu=0\n",
    "sigma=0.1\n",
    "\n",
    "\n",
    "## Flags\n",
    "Trained_M=False\n",
    "Vis_Data=False\n",
    "Sum_Data=False\n",
    "if Trained_M==False:\n",
    "    Vis_Data=True\n",
    "    Sum_Data=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Functions\n",
    "\n",
    "\n",
    "def Data_histogram_plot(dataset, label):\n",
    "    hist, bins = np.histogram(dataset, bins=n_classes)\n",
    "    width = 0.7 * (bins[1] - bins[0])\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.bar(center, hist, align='center', width=width)\n",
    "    plt.xlabel(label)\n",
    "    plt.ylabel(\"Image count\")\n",
    "    plt.show()\n",
    "\n",
    "def Data_view(dataset,labels,n_classes):\n",
    "    plt.rcParams['figure.figsize'] = (20.0, 20.0)\n",
    "    histo = [0 for i in range(n_classes)]\n",
    "    samples = {}\n",
    "    for idx, l in enumerate(labels):\n",
    "        histo[l] += 1\n",
    "        if l not in samples:\n",
    "            samples[l] = dataset[idx]\n",
    "    total_class = len(set(labels))\n",
    "    ncols = 4\n",
    "    nrows = 11\n",
    "\n",
    "    print(\"total tests {}. total labels: {}\".format(len(dataset), total_class))\n",
    "    _, axes = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "\n",
    "    class_idx = 0\n",
    "    for r in range(nrows):\n",
    "        for c in range(ncols):\n",
    "            a = axes[r][c]\n",
    "            a.axis('off')\n",
    "            if class_idx in samples:\n",
    "                cmap = 'gray' if len(samples[class_idx].shape) == 2 else None\n",
    "                a.imshow(samples[class_idx],cmap=cmap)\n",
    "                a.set_title(\"No.{} {}({})\".format(class_idx, signs[class_idx], histo[class_idx]), fontsize=13)\n",
    "            class_idx += 1\n",
    "    plt.rcParams['figure.figsize'] = figsize_default\n",
    "\n",
    "def gray_scale(image):\n",
    "   \n",
    "   \n",
    "    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "\n",
    "def image_normalize(image):\n",
    "#    image = np.divide(image, 255)\n",
    "    image = (image - image.mean())/image.std()\n",
    "    return image\n",
    "    \n",
    "def Preprocess_Data(dataset):\n",
    "    \n",
    "    gray_images = list(map(gray_scale, dataset))\n",
    "    normalized_images=np.zeros((len(gray_images),gray_images[0].shape[0],gray_images[0].shape[1]))\n",
    "    for i, img in enumerate(gray_images):\n",
    "        normalized_images[i] = image_normalize(img)\n",
    "    normalized_images = normalized_images[..., None]\n",
    "    return normalized_images\n",
    "    \n",
    "def conv_fn(input, in_WH, in_D, out_WH, out_D):\n",
    "\n",
    "    filter_WH = in_WH - out_WH + 1\n",
    "    strides = [1,1,1,1]\n",
    "    W = tf.Variable(tf.truncated_normal(shape=(filter_WH, filter_WH, in_D, out_D), \\\n",
    "                                        mean = mu, stddev = sigma))\n",
    "    b = tf.Variable(tf.zeros(out_D))\n",
    "    model = tf.nn.conv2d(input, W, strides=strides, padding='VALID') + b\n",
    "    return model\n",
    "    \n",
    "def linear_fn(input, in_WH, out_WH):\n",
    "    W = tf.Variable(tf.truncated_normal(shape=(in_WH, out_WH), \\\n",
    "                                        mean=mu, stddev = sigma))\n",
    "    b = tf.Variable(tf.zeros(out_WH))\n",
    "    model = tf.matmul(input, W) + b\n",
    "    return model\n",
    "    \n",
    "def Predict(X_data,logits,BATCH_SIZE=64):\n",
    "    num_examples = len(X_data)\n",
    "    y_pred = np.zeros(num_examples, dtype=np.int32)\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x = X_data[offset:offset+BATCH_SIZE]\n",
    "        y_pred[offset:offset+BATCH_SIZE] = sess.run(tf.argmax(logits, 1), \n",
    "                           feed_dict={x:batch_x, keep_prob:1, keep_prob_conv:1})\n",
    "    return y_pred\n",
    "    \n",
    "def evaluate(X_data, y_data, BATCH_SIZE=64):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, \n",
    "                            feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0, keep_prob_conv: 1.0 })\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Loading Data\n",
    "#Download the Data from https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/traffic-signs-data.zip\n",
    "\n",
    "training_file = \"./Data_Input/train.p\"\n",
    "validation_file= \"./Data_Input/valid.p\"\n",
    "testing_file = \"./Data_Input/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "# Mapping ClassID to traffic sign names\n",
    "signs = []\n",
    "with open('./Data_Input/signnames.csv', 'r') as csvfile:\n",
    "    signnames = csv.reader(csvfile, delimiter=',')\n",
    "    next(signnames,None)\n",
    "    for row in signnames:\n",
    "#        print(row)\n",
    "        signs.append(row[1])\n",
    "    csvfile.close()\n",
    "#print(signs)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################  \n",
    "## Dataset Summary\n",
    "\n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "#if Sum_Data==True:\n",
    "# Number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# Number of testing examples\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# Number of validation examples.\n",
    "n_validation = X_valid.shape[0]\n",
    "\n",
    "# What's the shape of an traffic sign image?\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "# How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Number of training examples: \", n_train)\n",
    "print(\"Number of testing examples: \", n_test)\n",
    "print(\"Number of validation examples: \", n_validation)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Data Exploratory Visualization\n",
    "\n",
    "if Vis_Data==True:\n",
    "    Data_view(X_train,y_train,len(np.unique(y_train)))\n",
    "    Data_histogram_plot(y_train, \"Training examples\")\n",
    "    Data_histogram_plot(y_valid, \"Validation examples\")\n",
    "    Data_histogram_plot(y_test, \"Test examples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Data Preprocessing\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "Preprocessed_data=Preprocess_Data(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################    \n",
    "## Model Architecture\n",
    "\n",
    "def LaNet(input):\n",
    "    ## Layer1\n",
    "    # convolution layer: 32x32x1 -> 28x28x6\n",
    "    conv1 = conv_fn(input, 32, 1, 28, 6)\n",
    "    conv1 = tf.nn.relu(conv1,name='convolution0')\n",
    "    # pooling: 28x28x12 -> 14x14x6\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID',name='convolution1')\n",
    "    \n",
    "    ## Layer2\n",
    "    # convolution layer 14x14x6 -> 10x10x16\n",
    "    conv2 = conv_fn(conv1, 14, 6, 10, 16)\n",
    "    conv2 = tf.nn.relu(conv2,name='convolution2')\n",
    "    conv2 = tf.nn.dropout(conv2, keep_prob_conv)\n",
    "    # pooling: 10x10x16 -> 5x5x16\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID',name='convolution3')\n",
    "    \n",
    "\n",
    "    \n",
    "    ## Layer3\n",
    "    # Flattening: Input = 5x5x16. Output = 400\n",
    "    flat = tf.contrib.layers.flatten(conv2)\n",
    "    flat = tf.nn.dropout(flat, keep_prob)\n",
    "    layer3 = linear_fn(flat, 400, 120)\n",
    "    layer3 = tf.nn.relu(layer3)\n",
    "    \n",
    "\n",
    "    \n",
    "    ## Layer4\n",
    "    layer4 = linear_fn(layer3, 120, 84)\n",
    "    layer4 = tf.nn.relu(layer4)\n",
    "    \n",
    "    ## Layer 5\n",
    "    output = linear_fn(layer4, 84, 43)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Model Training & Solution Approach\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "keep_prob = tf.placeholder(tf.float32) \n",
    "keep_prob_conv = tf.placeholder(tf.float32)\n",
    "LaNet_Logits=LaNet(x)\n",
    "one_hot_y = tf.one_hot(y, n_classes)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=LaNet_Logits, labels=one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "correct_prediction = tf.equal(tf.argmax(LaNet_Logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "model_file = './models'\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def train(X_data, y_data,X_val,y_val):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"starting ...\")\n",
    "        for i in range(EPOCHS):\n",
    "            X_train, y_train = shuffle(X_data, y_data)\n",
    "            for offset in range(0, len(X_train), BATCH_SIZE):\n",
    "                end = offset + BATCH_SIZE\n",
    "                features, labels = X_train[offset:end], y_train[offset:end]\n",
    "#                %sess.run(training_operation, feed_dict={x : features, y : labels, keep_prob: 0.8})\n",
    "                sess.run(training_operation, feed_dict={x: features, y: labels, keep_prob : 0.5, keep_prob_conv: 0.7})\n",
    "            validation_accuracy  = evaluate(X_val, y_val)\n",
    "            print(\"EPOCH {} : Validation Accuracy = {:.3f}%\".format(i+1, (validation_accuracy*100)))\n",
    "\n",
    "        saver.save(sess, model_file);\n",
    "        print(\"Model saved \")\n",
    "        \n",
    "X_val=Preprocess_Data(X_valid)\n",
    "train(Preprocessed_data,y_train,X_val,y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Test Dataset Model Architecture\n",
    "test_preprocessed=Preprocess_Data(X_test)\n",
    "with tf.Session() as sess:\n",
    "#    tf.reset_default_graph()\n",
    "    saver.restore(sess, model_file)\n",
    "    y_pred = Predict(test_preprocessed,LaNet_Logits)\n",
    "    test_accuracy = sum(y_test == y_pred)/len(y_test)\n",
    "    print(\"Test Accuracy = {:.1f}%\".format(test_accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Test a Model on New Images\n",
    "\n",
    "new_test_images = []\n",
    "path = './Test_Images/'\n",
    "for image in os.listdir(path):\n",
    "    img = cv2.imread(path + image)\n",
    "    img = cv2.resize(img, (32,32))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    new_test_images.append(img)\n",
    "new_IDs = [11, 1, 12, 18, 38]\n",
    "print(\"Number of new testing examples: \", len(new_test_images))\n",
    "\n",
    "plt.figure(figsize=(15, 16))\n",
    "for i in range(len(new_test_images)):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(new_test_images[i])\n",
    "    plt.xlabel(signs[new_IDs[i]])\n",
    "    plt.ylabel(\"New testing image\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.tight_layout(pad=0, h_pad=0, w_pad=0)\n",
    "plt.show()\n",
    "\n",
    "new_test_images_preprocessed = Preprocess_Data(np.asarray(new_test_images))\n",
    "\n",
    "def y_predict_model(Input_data, top_k=5):\n",
    "    \"\"\"\n",
    "    Generates the predictions of the model over the input data, and outputs the top softmax probabilities.\n",
    "        Parameters:\n",
    "            X_data: Input data.\n",
    "            top_k (Default = 5): The number of top softmax probabilities to be generated.\n",
    "    \"\"\"\n",
    "    num_examples = len(Input_data)\n",
    "    y_pred = np.zeros((num_examples, top_k), dtype=np.int32)\n",
    "    y_prob = np.zeros((num_examples, top_k))\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess,model_file)\n",
    "        y_prob, y_pred = sess.run(tf.nn.top_k(tf.nn.softmax(LaNet_Logits), k=top_k), \n",
    "                             feed_dict={x:Input_data, keep_prob:1, keep_prob_conv:1})\n",
    "    return y_prob, y_pred\n",
    "\n",
    "y_prob, y_pred = y_predict_model(new_test_images_preprocessed)\n",
    "\n",
    "test_accuracy = 0\n",
    "for i in enumerate(new_test_images_preprocessed):\n",
    "    accu = new_IDs[i[0]] == np.asarray(y_pred[i[0]])[0]\n",
    "    if accu == True:\n",
    "        test_accuracy += 0.2\n",
    "print(\"New Images Test Accuracy = {:.1f}%\".format(test_accuracy*100))\n",
    "\n",
    "plt.figure(figsize=(15, 16))\n",
    "for i in range(len(new_test_images_preprocessed)):\n",
    "    plt.subplot(5, 2, 2*i+1)\n",
    "    plt.imshow(new_test_images[i]) \n",
    "    plt.title(signs[y_pred[i][0]])\n",
    "    plt.axis('off')\n",
    "    plt.subplot(5, 2, 2*i+2)\n",
    "    plt.barh(np.arange(1, 6, 1), y_prob[i, :])\n",
    "    labels = [signs[j] for j in y_pred[i]]\n",
    "    plt.yticks(np.arange(1, 6, 1), labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Displaying intermediate outputs\n",
    "\n",
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n",
    "    # Here make sure to preprocess your image_input in a way your network expects\n",
    "    # with size, normalization, ect if needed\n",
    "    # image_input =\n",
    "    # Note: x should be the same name as your network's tensorflow data placeholder variable\n",
    "    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n",
    "    activation = tf_activation.eval(session=sess,feed_dict={x: new_test_images_preprocessed, keep_prob: 1.0, keep_prob_conv:1.0})\n",
    "    featuremaps = activation.shape[3]\n",
    "    plt.figure(plt_num, figsize=(25,25))\n",
    "    for featuremap in range(featuremaps):\n",
    "        plt.subplot(8,8, featuremap+1) # sets the number of feature maps to show on each row and column\n",
    "        plt.title('FeatureMap ' + str(featuremap)) # displays the feature map number\n",
    "        if activation_min != -1 & activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin =activation_min, vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmax=activation_max, cmap='rainbow')\n",
    "        elif activation_min !=-1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, cmap='rainbow')\n",
    "        else:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", cmap='rainbow')\n",
    "            \n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    conv1 = sess.graph.get_tensor_by_name('convolution0:0')\n",
    "    outputFeatureMap(new_test_images_preprocessed,conv1, activation_min=-1, activation_max=-1, plt_num=1)\n",
    "    \n",
    "    conv1 = sess.graph.get_tensor_by_name('convolution1:0')\n",
    "    outputFeatureMap(new_test_images_preprocessed,conv1, activation_min=-1, activation_max=-1, plt_num=2)\n",
    "    \n",
    "    conv2 = sess.graph.get_tensor_by_name('convolution2:0')\n",
    "    outputFeatureMap(new_test_images_preprocessed,conv2, activation_min=-1, activation_max=-1, plt_num=3)\n",
    "    \n",
    "    conv2 = sess.graph.get_tensor_by_name('convolution3:0')\n",
    "    outputFeatureMap(new_test_images_preprocessed,conv2, activation_min=-1, activation_max=-1, plt_num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
